{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22486881",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#In windows, run anaconda prompt, and go to BIDS folder. Run jupyter notebook\n",
    "#MAIN CODE TO CALCULATE SEED BASED VOXEL IN BATCH MODE FOLLOWING BIDS AND fMRIPREP\n",
    "\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import bids\n",
    "import nibabel as nib\n",
    "from scipy.stats import pearsonr\n",
    "import statsmodels.api as sm\n",
    "from nilearn import plotting as nplot\n",
    "from nilearn import image as nimg\n",
    "from nilearn.image import resample_to_img\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import pandas as pd\n",
    "#%%\n",
    "##\n",
    "def parse():\n",
    "\n",
    "        options = argparse.ArgumentParser(description=\"Run 1st level analysis. Created by ...\")\n",
    "        options.add_argument('-p', '--participants', nargs='+',dest=\"participants\", action='store', type=str, required=False,\n",
    "                            help='id of subject or list of subjects')\n",
    "        options.set_defaults(participants=None)\n",
    "        options.add_argument('-w', '--workdir',dest=\"workdir\", action='store', type=str, required=False,\n",
    "                            help='the work directory for the project')\n",
    "        options.set_defaults(workdir=os.environ[\"ROOTDIR\"])\n",
    "        options.add_argument('-b', '--bidsdir',dest=\"rawdata\", action='store', type=str, required=False,\n",
    "                            help='the work directory for the project')\n",
    "        options.set_defaults(rawdata=os.path.join(os.environ[\"ROOTDIR\"],\"BIDS\"))\n",
    "        options.add_argument('-d', '--derivatives',dest=\"derivatives\", action='store', type=str, required=True,\n",
    "                            help='path to fMRIprep directory')\n",
    "        options.add_argument('-o', '--outputs',dest=\"output\", action='store', type=str, required=True,\n",
    "                            help='path to fMRIprep directory')\n",
    "        #print(options.parse_args())\n",
    "\n",
    "        return options.parse_args()\n",
    "\n",
    "#%%\n",
    "def main ():\n",
    "    os.environ[\"ROOTDIR\"] = 'D:/BIDS/'  # seth path\n",
    "    rootdir = os.environ[\"ROOTDIR\"]\n",
    "    if hasattr(sys, \"ps1\"):\n",
    "        options = {}\n",
    "        workdir = os.environ[\"ROOTDIR\"]\n",
    "        rawdata = os.path.join(workdir,\"analysis\")\n",
    "        derivat = os.path.join(workdir,\"derivatives\",\"fmriprep\")\n",
    "        output  = os.path.join(rawdata,\"firstlevel\")\n",
    "        masks = os.path.join(rawdata,\"masks\")\n",
    "        participants = []\n",
    "\n",
    "    else :\n",
    "        options = parse()\n",
    "        participants = options.participants\n",
    "        workdir = options.workdir\n",
    "        rawdata = options.rawdata\n",
    "        derivat = options.derivatives\n",
    "        output  = options.output\n",
    "        script_path = os.path.dirname(__file__)\n",
    "        masks = os.path.join(script_path,\"masks\")\n",
    "\n",
    "    seednames = ['DCPutamen_space-MNI152NLin2009cAsym.nii.gz',\n",
    "                 'DorsalCaudate_space-MNI152NLin2009cAsym.nii.gz',\n",
    "                 'InfVentralCaudate_space-MNI152NLin2009cAsym.nii.gz',\n",
    "                 'SupVentralCaudate_space-MNI152NLin2009cAsym.nii.gz',\n",
    "                 'VRPutamen_space-MNI152NLin2009cAsym.nii.gz',\n",
    "                 'DRPutamen_space-MNI152NLin2009cAsym.nii.gz']\n",
    "\n",
    "    print('Rawdata: ', rawdata)\n",
    "\n",
    "    #################\n",
    "    #GM + Thalamus mask to perform a seed to GMvoxel analysis\n",
    "    Vgm_img = nib.load(os.path.join(masks,'GrayMattermask_thalamus_space-MNI152NLin2009cAsym.nii.gz'))\n",
    "\n",
    "    \n",
    "    #extract data array\n",
    "    Vgm = Vgm_img.get_fdata()\n",
    "    #save origianl dimensions (voxels_x, voxels_y, voxels_z)\n",
    "    dim3d = Vgm.shape\n",
    "    #reshape to 2D\n",
    "    Vgm = Vgm.reshape(-1, np.prod(dim3d))  # -1 means auto-calculate size of dimension\n",
    "    #save indexes in which Vgm == 1 (indexes for gray matter location)\n",
    "    idx_GM = np.where(Vgm)[1]\n",
    "\n",
    "    bidslayout = bids.BIDSLayout(derivat,derivatives=True, validate = False) #With validate = True it doesn't find any subjects\n",
    "\n",
    "    if not participants:\n",
    "        participants = bidslayout.get_subjects()\n",
    "    for i in  participants:\n",
    "        print(\"Subject: \", i)\n",
    "        i = i.replace(\"sub-\", \"\")\n",
    "        for ses in bidslayout.get_sessions(subject=i):\n",
    "            print(\"Session: \", ses)\n",
    "            #for multi-echo files, apply transform to standard space\n",
    "            for r in bidslayout.get_runs(subject=i, session=ses):\n",
    "                print(\"Run:\", r)\n",
    "                filesrest = bidslayout.get(subject=i,\n",
    "                                 session=ses,\n",
    "                                 run=r,\n",
    "                                 extension=\".nii.gz\",\n",
    "                                 suffix=\"bold\",\n",
    "                                 space=\"MNI152NLin2009cAsym\",\n",
    "                                 #regex_search=True,\n",
    "                                 #invalid_filters=\"allow\"\n",
    "                                 )\n",
    "\n",
    "                print(\"Filerest:\", filesrest)\n",
    "                if len(filesrest)>1:\n",
    "                    print(f\"More than one file match bold filters. Subject {i}, ses {ses}, run {r}\")\n",
    "                    continue\n",
    "                elif len(filesrest)<1:\n",
    "                    print(f\"No file match bold filters. Subject {i}, ses {ses}, run {r}\")\n",
    "                    continue\n",
    "                bold = filesrest[0]\n",
    "\n",
    "                filesbrainmask = bidslayout.get(subject=i,\n",
    "                                  session=ses,\n",
    "                                  run=r,\n",
    "                                  task='rest',\n",
    "                                  extension=\".nii.gz\",\n",
    "                                  suffix=\"mask\",\n",
    "                                  space=\"MNI152NLin2009cAsym\",\n",
    "                                  #regex_search=True,\n",
    "                                  invalid_filters=\"allow\"\n",
    "                                  )\n",
    "                print(\"Brain_Mask:\", filesbrainmask)\n",
    "                if len(filesbrainmask)>1:\n",
    "                    print(f\"More than one file match brainmask filters. Subject {i}, ses {ses}, run {r}\")\n",
    "                    continue\n",
    "                brainmask = filesbrainmask[0]\n",
    "\n",
    "\n",
    "                filesanat = bidslayout.get(subject=i,\n",
    "                                 session=ses,\n",
    "                                 run=r,\n",
    "                                 extension=\".nii.gz\",\n",
    "                                 suffix=\"T1w\",\n",
    "                                 #desc=\"preproc\",\n",
    "                                 space=\"MNI152NLin2009cAsym\",\n",
    "                                 #regex_search=True,\n",
    "                                 invalid_filters=\"allow\"\n",
    "                                 )\n",
    "\n",
    "                print(f\"Anat: {filesanat} \")\n",
    "                if len(filesrest)>1:\n",
    "                    print(f\"More than one file match anat filters. Subject {i}, ses {ses}, run {r}\")\n",
    "                    continue\n",
    "                #anat = filesanat[0]\n",
    "\n",
    "                #subj_i=participants.index(i)\n",
    "                t = np.zeros((len(idx_GM), len(seednames)))\n",
    "\n",
    "                #Load Confounds\n",
    "                confounds_ents = {}\n",
    "                confounds_ents[\"subject\"] = bold.entities['subject']\n",
    "                confounds_ents[\"session\"] = bold.entities['session']\n",
    "                #confounds_ents[\"acquisition\"] = bold.entities['acquisition']\n",
    "                confounds_ents[\"run\"] = bold.entities['run']\n",
    "                confounds_ents[\"task\"] = bold.entities['TaskName']\n",
    "                #confounds_ents['desc'] = \"confounds\"\n",
    "                confounds_ents['suffix'] = \"timeseries\"\n",
    "                confounds_ents['extension'] = \".tsv\"\n",
    "                confounds = bidslayout.get(return_type='file', **confounds_ents, invalid_filters=\"allow\")[0]\n",
    "                allconfounds_df = pd.read_csv(confounds, sep='\\t')\n",
    "\n",
    "                melodic_ents = confounds_ents\n",
    "                melodic_ents['suffix'] = \"mixing\"\n",
    "                melodic = bidslayout.get(return_type='file', **melodic_ents, invalid_filters=\"allow\")[0]\n",
    "                melodic_df = pd.read_csv(melodic, sep='\\t')\n",
    "\n",
    "                aroma_noise = confounds_ents\n",
    "                aroma_noise['suffix'] = \"AROMAnoiseICs\"\n",
    "                aroma_noise['extension'] = \".csv\"\n",
    "                aroma_noise = bidslayout.get(return_type='file', **aroma_noise, invalid_filters=\"allow\")[0]\n",
    "                aroma_noise_idx = np.genfromtxt(aroma_noise, delimiter=',').astype(int)\n",
    "\n",
    "                aroma_conf = melodic_df.iloc[:,aroma_noise_idx-1]\n",
    "\n",
    "                conf_index= ['csf', 'white_matter','cosine00','cosine01','cosine02','cosine03']\n",
    "                confounds_df = allconfounds_df[conf_index]\n",
    "\n",
    "                #DROP 4 initial volumes and generate confounds matrix\n",
    "                drop_confound_df = confounds_df.loc[4:].values #from confounds\n",
    "                drop_aroma_conf = aroma_conf.loc[3:].values #it has one less time point\n",
    "\n",
    "                try:\n",
    "                    my_confounds = np.concatenate((drop_confound_df, drop_aroma_conf), axis=1)\n",
    "\n",
    "                    #DROP 4 volumes from bold data\n",
    "                    raw_func_img = nimg.load_img(bold)\n",
    "                    func_img = raw_func_img.slicer[:,:,:,4:]\n",
    "                    dim4d = func_img.shape\n",
    "                    N = dim4d[-1]  # number of time points\n",
    "    \n",
    "                    # #reshape into 2d\n",
    "                    func_2d = func_img.get_fdata().reshape(-1, func_img.shape[-1]).T\n",
    "    \n",
    "                    # pre-allocate space for seeds' time series\n",
    "                    TS_seeds = np.empty((N, len(seednames)))\n",
    "                    # Input new nifti for seeds\n",
    "                    # Compute SEEDs TS\n",
    "                    for k in range(len(seednames)):\n",
    "                        print('----- TS Seed:', seednames[k], '-----')\n",
    "    \n",
    "                        seed = os.path.join(masks, seednames[k])\n",
    "                        #load seed image\n",
    "                        Vseed_img = nib.load(seed)\n",
    "                        Vseed = Vseed_img.get_fdata().reshape(func_2d.shape[-1])\n",
    "    \n",
    "                        #for each time point, compute mean of voxels in which seed==1\n",
    "                        for vol in range(N):\n",
    "                            Vaux = func_2d[vol, :]\n",
    "                            TS_seeds[vol, k] = np.mean(Vaux[Vseed == 1])\n",
    "                            del Vaux\n",
    "    \n",
    "                        del Vseed\n",
    "    \n",
    "                    # 1st Level Analysis: TSvox = b0 + b1*TSseed1 + b2*TSseed2 + ... + b6*TSseed6 + confounds\n",
    "                    print('First level analysis -----')\n",
    "                    print('TSvox = b0 + b1*TSseed1 + b2*TSseed2 + ... + b6*TSseed6 + CSF + WM + cosines + AROMA_noiseICs')\n",
    "    \n",
    "                    # Add a constant term to the independent variables\n",
    "                    design_matrix = np.concatenate((TS_seeds,my_confounds), axis=1)\n",
    "                    design_matrix = sm.add_constant(design_matrix)\n",
    "    \n",
    "                    for vox in range(len(idx_GM)):\n",
    "                        idx_fullvol = idx_GM[vox]\n",
    "                        # Fit the multiple linear regression model\n",
    "                        model = sm.OLS(func_2d[:, idx_fullvol], design_matrix)\n",
    "                        results = model.fit()\n",
    "                        # Get the t-values\n",
    "                        t_values = results.tvalues\n",
    "                        t[vox, :] = t_values[1:7]   # t[voxels,seeds]\n",
    "    \n",
    "                    #SAVE tmaps as nifti files\n",
    "    \n",
    "                    #Generate T-map using brainmask affine info\n",
    "                    Vgm_img=nimg.load_img(brainmask)\n",
    "                    tmap_img = Vgm_img\n",
    "                    tmap = tmap_img.get_fdata()\n",
    "    \n",
    "                    for t_index in range(t.shape[1]):\n",
    "                        tmap = tmap.reshape(-1, np.prod(dim3d)) #reshape to 2D\n",
    "                        tmap[:] = 0 # clean img\n",
    "                        tmap[0,idx_GM] = t[:,t_index] #put t-values for each seed; seed 1 (t[:,0])\n",
    "                        tmap = tmap.reshape(dim3d) #reshape to 3D\n",
    "    \n",
    "                        #Generate nifti object\n",
    "                        tmap_img = nib.Nifti1Image(tmap, tmap_img.affine)\n",
    "    \n",
    "                        tmap_metadata = {\n",
    "                            'task': 'rest',\n",
    "                            'suffix': 'tstat',\n",
    "                            'extension': '.nii.gz',\n",
    "                            'run': str(r),\n",
    "                            'session': str(ses),\n",
    "                            'subject': 'sub-'+i,\n",
    "                            'space': 'MNI152NLin2009cAsym',\n",
    "                            'seed': seednames[t_index].split(\"_\")[0]\n",
    "                        }\n",
    "    \n",
    "                        ##uncomment to plot tmap on top of T1 image\n",
    "                        #t1_img = nib.load(anat)\n",
    "                        #display=nplot.plot_anat(t1_img)\n",
    "                        #display.add_overlay(tmap_img)\n",
    "    \n",
    "                        # Save each tmap as a nifti file\n",
    "                        filepath = os.path.join(output,tmap_metadata[\"subject\"],\"ses-\"+tmap_metadata['session'],\"func\")\n",
    "                        filename = tmap_metadata[\"subject\"] + \"_\" + \\\n",
    "                                    \"ses-\"+tmap_metadata['session'] + \"_\" + \\\n",
    "                                    \"task-\"+tmap_metadata['task'] + \"_\" + \\\n",
    "                                    \"run-\"+tmap_metadata['run'] + \"_\" + \\\n",
    "                                    \"space-\"+tmap_metadata['space'] + \"_\" + \\\n",
    "                                    \"seed-\"+tmap_metadata['seed'] + \"_\" + \\\n",
    "                                    tmap_metadata[\"suffix\"] + \\\n",
    "                                    tmap_metadata[\"extension\"]\n",
    "    \n",
    "                        folder_subj=os.path.join(output,tmap_metadata[\"subject\"])\n",
    "                        if not os.path.exists(folder_subj):\n",
    "                            os.makedirs(folder_subj)\n",
    "    \n",
    "                        if not os.path.exists(filepath):\n",
    "                             os.makedirs(filepath)\n",
    "    \n",
    "                        tmap_img.to_filename(os.path.join(filepath, filename))\n",
    "    \n",
    "                    print('\\t\\t ----- Finished subject', i, ' Sess: ' ,ses,' Run: ',r,' ----')\n",
    "                    \n",
    "                except:\n",
    "                    print('\\t\\t ----- ERRO NO SUJEITO', i, ' Sess: ' ,ses,' Run: ',r,' ----')\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f22ba7-7238-48ad-bd15-de7d66ef7387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
