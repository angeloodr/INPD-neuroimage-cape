{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Angelo Orletti Del Rey for its masters in psychiatry at Unifesp - SP - Brasil\n",
    "\n",
    "It was based in our groups previeous works in cotical striatal connectivity in INPD data base of fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\angel\\Documents\\INPD_neuroimages_CAPE\\.venv_fmriINPD\\lib\\site-packages\\nilearn\\__init__.py:67: FutureWarning: Python 3.7 support is deprecated and will be removed in release 0.12 of Nilearn. Consider switching to Python 3.9 or 3.10.\n",
      "  _python_deprecation_warnings()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import bids\n",
    "import nibabel as nib\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import chi2\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from nilearn import plotting as nplot\n",
    "from nilearn import image as nimg\n",
    "from nilearn.image import resample_to_img\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "from scipy.linalg import LinAlgError\n",
    "\n",
    "# Suppress convergence warnings\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"statsmodels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firstlevel:  C://Users//angel\\Documents//INPD_neuroimages_CAPE\\BIDS\\derivatives\\first_level_results\n",
      "Loading tmaps for all participants...\n"
     ]
    }
   ],
   "source": [
    "def parse():\n",
    "\n",
    "        options = argparse.ArgumentParser(description=\"Run 1st level analysis. Created by ...\")\n",
    "        options.add_argument('-p', '--participants', nargs='+',dest=\"participants\", action='store', type=str, required=False,\n",
    "                            help='id of subject or list of subjects')\n",
    "        options.set_defaults(participants=None)\n",
    "        options.add_argument('-w', '--workdir',dest=\"workdir\", action='store', type=str, required=False,\n",
    "                            help='the work directory for the project')\n",
    "        options.set_defaults(workdir=os.environ[\"ROOTDIR\"])\n",
    "        options.add_argument('-b', '--bidsdir',dest=\"rawdata\", action='store', type=str, required=False,\n",
    "                            help='the work directory for the project')\n",
    "        options.set_defaults(rawdata=os.path.join(os.environ[\"ROOTDIR\"],\"BIDS\"))\n",
    "        options.add_argument('-d', '--derivatives',dest=\"derivatives\", action='store', type=str, required=True,\n",
    "                            help='path to fMRIprep directory')\n",
    "        options.add_argument('-o', '--outputs',dest=\"output\", action='store', type=str, required=True,\n",
    "                            help='path to fMRIprep directory')\n",
    "        #print(options.parse_args())\n",
    "\n",
    "        return options.parse_args()\n",
    "\n",
    "## CHANGE HERE THE PATH TO YOUR DATA\n",
    "os.environ[\"ROOTDIR\"] = r'C://Users//angel\\Documents//INPD_neuroimages_CAPE'   # seth path\n",
    "rootdir = os.environ[\"ROOTDIR\"]\n",
    "if hasattr(sys, \"ps1\"):\n",
    "    options = {}\n",
    "    workdir = os.environ[\"ROOTDIR\"]\n",
    "    firstleveldir  = os.path.join(workdir,\"BIDS\",\"derivatives\",\"first_level_results\")\n",
    "    demographic = os.path.join(workdir,\"metadata\")\n",
    "    confounds = os.path.join(workdir,\"metadata\")\n",
    "    output = os.path.join(workdir,'sensitivity_analisys_second_level')\n",
    "    participants = []\n",
    "\n",
    "else :\n",
    "    options = parse()\n",
    "    participants = options.participants\n",
    "    workdir = options.workdir\n",
    "    rawdata = options.rawdata\n",
    "    derivat = options.derivatives\n",
    "    output  = options.outputs\n",
    "\n",
    "print('firstlevel: ', firstleveldir)\n",
    "    \n",
    "bidslayout = bids.BIDSLayout(firstleveldir, validate = False) #With validate = True it doesn't find any subjects\n",
    "\n",
    "if not participants:\n",
    "    participants = bidslayout.get_subjects() #take all subjects\n",
    "    #participants_sub = [\"sub-\" + item for item in participants]\n",
    "\n",
    "seednames = ['DCPutamen',\n",
    "            'DorsalCaudate',\n",
    "            'DRPutamen',\n",
    "            'InfVentralCaudate',\n",
    "            'SupVentralCaudate',\n",
    "            'VRPutamen'\n",
    "            ]\n",
    "\n",
    "tmap_allsubj = [] #initialize array for tmaps\n",
    "\n",
    "print(\"Loading tmaps for all participants...\")\n",
    "for p in participants:\n",
    "    #print(f\"Subject: {p}\")\n",
    "    p = p.replace(\"sub-\", \"\")\n",
    "\n",
    "    # for ses in bidslayout.get_sessions(subject=p):\n",
    "    #     #print(f\"Session: {ses}\")                \n",
    "    \n",
    "    ses = '1'\n",
    "\n",
    "    for r in bidslayout.get_runs(subject=p, session=ses):\n",
    "        #print(f\"Run: {r}\")\n",
    "\n",
    "        tmap_metadata = {\n",
    "            'task': 'rest',\n",
    "            'suffix': 'tstat',\n",
    "            'extension': '.txt',\n",
    "            'run': str(r),\n",
    "            'session': str(ses),\n",
    "            'subject': 'sub-'+p,\n",
    "            'space': 'MNI152NLin2009cAsym',\n",
    "            'seed': 'SeedtoROI'\n",
    "        }\n",
    "\n",
    "        # Save each tmap as a nifti file\n",
    "        filepath = os.path.join(firstleveldir,tmap_metadata[\"subject\"],\"ses-\"+tmap_metadata['session'],\"func\")\n",
    "        filename = tmap_metadata[\"subject\"] + \"_\" + \\\n",
    "                    \"ses-\"+tmap_metadata['session'] + \"_\" + \\\n",
    "                    \"task-\"+tmap_metadata['task'] + \"_\" + \\\n",
    "                    \"run-\"+tmap_metadata['run'] + \"_\" + \\\n",
    "                    \"space-\"+tmap_metadata['space'] + \"_\" + \\\n",
    "                    \"seed-\"+tmap_metadata['seed'] + \"_\" + \\\n",
    "                    tmap_metadata[\"suffix\"] + \\\n",
    "                    tmap_metadata[\"extension\"]\n",
    "        tmap_path = os.path.join(filepath, filename)\n",
    "\n",
    "        #load the tmaps for each subject and then append to the tmap_allsubj array\n",
    "        tmap = pd.read_csv(tmap_path, delimiter=' ', header=None, skiprows=1) #also skips the first row\n",
    "        tmap.columns = seednames\n",
    "        tmap_allsubj.append(tmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading socioeconomic and psichometric data for all participants...\n",
      "loading the p factor scores\n",
      "Merging the two dataframes\n"
     ]
    }
   ],
   "source": [
    "#load socioeconomic and psichometric data for all subjects\n",
    "print(\"Loading socioeconomic and psichometric data for all participants...\")\n",
    "socio_psi=pd.read_csv(os.path.join(demographic,'variaveis_analise_conf_nova-exclus√£o.tsv'), sep='\\t')\n",
    "#substitute in age colunm , to . for decimals\n",
    "socio_psi['age'] = socio_psi['age'].str.replace(',', '.').astype(float)\n",
    "\n",
    "print('loading the p factor scores')\n",
    "p_factor = pd.read_excel(os.path.join(demographic,'factor-loading-angelo.xlsx'))\n",
    "\n",
    "print('Merging the two dataframes')\n",
    "# correct the collumn ident of p_factor to have the format 'sub-XXXXX'\n",
    "p_factor['ident'] = p_factor['ident'].apply(lambda x: f'sub-{int(x):05d}')\n",
    "# rename the collumn ident to sub_id\n",
    "p_factor = p_factor.rename(columns={'ident': 'sub_id'})\n",
    "# merge the two dataframes on sub_id\n",
    "socio_psi = pd.merge(socio_psi, p_factor[['sub_id', 'cl_p_fl']], on='sub_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organizing tmaps...\n",
      "Joining tmaps with socioeconomic and psichometric data...\n",
      "Running regression analysis (salience and defualt mode network)...\n",
      "running for cl_p_fl e seed DCPutamen\n",
      "running for cl_p_fl e seed DorsalCaudate\n",
      "running for cl_p_fl e seed DRPutamen\n",
      "running for cl_p_fl e seed InfVentralCaudate\n",
      "running for cl_p_fl e seed SupVentralCaudate\n",
      "running for cl_p_fl e seed VRPutamen\n"
     ]
    }
   ],
   "source": [
    "# organinzing the tmaps. Joining the columns of the tmaps for all subjects. So for each subject\n",
    "# we have to join the first collunm of the first tmap with the first column of the second tmap and so on\n",
    "# and then join the second column of the first tmap with the second column of the second tmap and so on\n",
    "# and so on\n",
    "print(\"Organizing tmaps...\")\n",
    "tmap_allsubj = pd.concat(tmap_allsubj, axis=1)\n",
    "tmap_allsubj_organized = {}\n",
    "\n",
    "for seed in seednames:\n",
    "    #drop columns with names different from seednames[i]\n",
    "    tmap_allsubj_organized[seed] = tmap_allsubj.drop(columns=[col for col in tmap_allsubj.columns if col != seed])\n",
    "\n",
    "# What do we have now: tmap_allsubj_organized is a dictionary with keys being the seednames and values being\n",
    "# dataframes with the tmaps for all subjects for that seed. This have dimensions of n_ROIs x n_subjects\n",
    "# Now we have to transpose it to get the data in the format n_subjects x n_ROIs and then join with the\n",
    "# socioeconomic and psichometric data\n",
    "# Also, we have to add the sub_id column to the conectivity data to be able to join with the socioeconomic\n",
    "# and psichometric data\n",
    "subs = {'sub_id': []}\n",
    "for p in participants:\n",
    "    ses = '1'\n",
    "    if bidslayout.get_runs(subject=p, session=ses) != []:\n",
    "        subs['sub_id'].append('sub-'+p)\n",
    "\n",
    "subs = pd.DataFrame(subs)\n",
    "\n",
    "# create the collunms names for the data frame of rois\n",
    "roi_names = [f'roi_{i}' for i in range(101)]\n",
    "\n",
    "for seed in seednames:\n",
    "    tmap_allsubj_organized[seed] = tmap_allsubj_organized[seed].transpose()\n",
    "    tmap_allsubj_organized[seed].columns = roi_names\n",
    "    tmap_allsubj_organized[seed] = tmap_allsubj_organized[seed].reset_index().join(subs, how='inner')\n",
    "    tmap_allsubj_organized[seed] = tmap_allsubj_organized[seed].drop(columns=['index'])\n",
    "\n",
    "# Now we have to join the tmaps with the socioeconomic and psichometric data\n",
    "print(\"Joining tmaps with socioeconomic and psichometric data...\")\n",
    "\n",
    "for seed in seednames:\n",
    "    tmap_allsubj_organized[seed] = pd.merge(tmap_allsubj_organized[seed], socio_psi, on='sub_id', how='inner')\n",
    "    tmap_allsubj_organized[seed] = tmap_allsubj_organized[seed].drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# now we run the regression analysis for each seed. The dependent variable is the connectivity and the independent\n",
    "# variables are the CAPE scores. The socioeconomic variables are used as confund variables\n",
    "print(\"Running regression analysis (salience and defualt mode network)...\")\n",
    "\n",
    "shaeffer_network = pd.read_csv('tpl-MNI152NLin2009cAsym_atlas-Schaefer2018_desc-100Parcels7Networks_dseg.tsv', delimiter='\\t')\n",
    "\n",
    "#extract the index with the name with SalVentAttn - salience network\n",
    "salience_index = shaeffer_network[shaeffer_network['name'].str.contains('SalVentAttn')]\n",
    "#extract the index with the name with Default - default mode network\n",
    "default_index = shaeffer_network[shaeffer_network['name'].str.contains('Default')]\n",
    "\n",
    "# running the regression analysis for each roi in the salience network\n",
    "results_salience, results_default = {}, {}\n",
    "for seed in seednames:\n",
    "    print(f'running for cl_p_fl e seed {seed}')\n",
    "    results_salience[seed], results_default[seed] = {}, {}\n",
    "\n",
    "    #salience analisys\n",
    "    for roi in salience_index['index']:\n",
    "\n",
    "        #building my strign for the fomrula\n",
    "        formula_salience = f'cl_p_fl ~ roi_{roi} + age + gender + abepscore + colection_site + fd_count_high_value'\n",
    "\n",
    "        #running the regression analysis\n",
    "        model_salience = smf.ols(formula_salience, data=tmap_allsubj_organized[seed])\n",
    "        fit_salience = model_salience.fit()\n",
    "\n",
    "        # Extract desired values\n",
    "        results_salience[seed][roi] = {\n",
    "            'coef': fit_salience.params[f'roi_{roi}'],\n",
    "            'intercept': fit_salience.params['Intercept'],\n",
    "            't_value': fit_salience.tvalues[f'roi_{roi}'],\n",
    "            'p_value': fit_salience.pvalues[f'roi_{roi}'],\n",
    "            'R_squared': fit_salience.rsquared,\n",
    "            'Adj_R_squared': fit_salience.rsquared_adj,\n",
    "            'AIC': fit_salience.aic,\n",
    "            'BIC': fit_salience.bic\n",
    "        }\n",
    "        \n",
    "    #DMN analisys\n",
    "    for roi in default_index['index']:\n",
    "\n",
    "        #building my strign for the fomrula\n",
    "        formula_default = f'cl_p_fl ~ roi_{roi} + age + gender + abepscore + colection_site + fd_count_high_value'\n",
    "        \n",
    "        # Run the regression analysis\n",
    "        model_default = smf.ols(formula_default, data=tmap_allsubj_organized[seed])\n",
    "        fit_default = model_default.fit()\n",
    "\n",
    "        # Extract desired values\n",
    "        results_default[seed][roi] = {\n",
    "            'coef': fit_default.params[f'roi_{roi}'],\n",
    "            'intercept': fit_default.params['Intercept'],\n",
    "            't_value': fit_default.tvalues[f'roi_{roi}'],\n",
    "            'p_value': fit_default.pvalues[f'roi_{roi}'],\n",
    "            'R_squared': fit_default.rsquared,\n",
    "            'Adj_R_squared': fit_default.rsquared_adj,\n",
    "            'AIC': fit_default.aic,\n",
    "            'BIC': fit_default.bic\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salience - DCPutamen\n",
      "DMN - DCPutamen\n",
      "Salience - DorsalCaudate\n",
      "DMN - DorsalCaudate\n",
      "Salience - DRPutamen\n",
      "DMN - DRPutamen\n",
      "Salience - InfVentralCaudate\n",
      "DMN - InfVentralCaudate\n",
      "Salience - SupVentralCaudate\n",
      "DMN - SupVentralCaudate\n",
      "Salience - VRPutamen\n",
      "DMN - VRPutamen\n"
     ]
    }
   ],
   "source": [
    "#correcting the p-values\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "for seed in seednames:\n",
    "    print(f'Salience - {seed}')\n",
    "    p_values = []\n",
    "\n",
    "    for roi in results_salience[seed].keys():\n",
    "        # print(roi)\n",
    "        # Extract the p-values for the current seed and roi\n",
    "        p_values.append(results_salience[seed][roi]['p_value'])\n",
    "\n",
    "    #correct the p-values using the Benjamini-Hochberg method\n",
    "    corrected_p_values = multipletests(p_values, method='fdr_bh')[1]\n",
    "\n",
    "    # Update the results with the corrected p-values\n",
    "    for index, roi in enumerate(results_salience[seed].keys()):\n",
    "        results_salience[seed][roi]['corrected_p_value'] = corrected_p_values[index]\n",
    "\n",
    "    print(f'DMN - {seed}')\n",
    "    p_values = []\n",
    "\n",
    "    for roi in results_default[seed].keys():\n",
    "        # print(roi)\n",
    "        # Extract the p-values for the current seed and roi\n",
    "        p_values.append(results_default[seed][roi]['p_value'])\n",
    "\n",
    "    #correct the p-values using the Benjamini-Hochberg method\n",
    "    corrected_p_values = multipletests(p_values, method='fdr_bh')[1]\n",
    "\n",
    "    # Update the results with the corrected p-values\n",
    "    for index, roi in enumerate(results_default[seed].keys()):\n",
    "        # print(f'index {index} roi {roi}')\n",
    "        results_default[seed][roi]['corrected_p_value'] = corrected_p_values[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting results to csv files...\n"
     ]
    }
   ],
   "source": [
    "# exporting the results to a csv file\n",
    "print(\"Exporting results to csv files...\")\n",
    "\n",
    "for seed in seednames:\n",
    "    for roi in results_salience[seed].keys():\n",
    "        results_salience[seed][roi] = pd.DataFrame(results_salience[seed][roi],index=[0])\n",
    "        results_salience[seed][roi].to_csv(os.path.join(output, f'cl_p_fl-{seed}-roi_{roi}_2ndlvl.csv'))\n",
    "    for roi in results_default[seed].keys():\n",
    "        results_default[seed][roi] = pd.DataFrame(results_default[seed][roi],index=[0])\n",
    "        results_default[seed][roi].to_csv(os.path.join(output, f'cl_p_fl-{seed}-roi_{roi}_2ndlvl.csv'))\n",
    "    tmap_allsubj_organized[seed].to_csv(os.path.join(output,f'{seed}_data-2ndlvl.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMN seed-DCPutamen \n",
      "DMN seed-DorsalCaudate \n",
      "DMN seed-DRPutamen \n",
      "DMN seed-InfVentralCaudate \n",
      "DMN seed-SupVentralCaudate \n",
      "DMN seed-VRPutamen \n",
      "salience seed-DCPutamen \n",
      "salience seed-DorsalCaudate \n",
      "salience seed-DRPutamen \n",
      "salience seed-InfVentralCaudate \n",
      "salience seed-SupVentralCaudate \n",
      "salience seed-VRPutamen \n"
     ]
    }
   ],
   "source": [
    "for seed in seednames:\n",
    "    print(f\"DMN seed-{seed} \")\n",
    "    for roi in results_default[seed].keys():\n",
    "        if results_default[seed][roi][\"corrected_p_value\"].item() <= 0.05:\n",
    "            print(f\"roi-{roi} -> corr_p_val {results_default[seed][roi]['corrected_p_value']}\")\n",
    "\n",
    "\n",
    "for seed in seednames:\n",
    "    print(f\"salience seed-{seed} \")\n",
    "    for roi, res in results_salience[seed].items():\n",
    "        val = res.get('corrected_p_value')\n",
    "        if val is not None and val.item() <= 0.05:\n",
    "            print(f\"roi-{roi} -> corr_p_val {val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra code - maybe removed in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_fmriINPD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
